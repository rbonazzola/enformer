{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_files(query_regions):\n",
    "    \n",
    "    mapping = {}\n",
    "    for REGION_INDEX in range(len(query_regions)):\n",
    "      chrom  = query_regions[REGION_INDEX][0]\n",
    "      posmin = query_regions[REGION_INDEX][1] - 131071\n",
    "      posmax = query_regions[REGION_INDEX][2] + 131072\n",
    "      mapping[REGION_INDEX] = f\"/grand/TFXcan/imlab/data/enformer_training_data/larger_window/human/chr{chrom}_{posmin}_{posmax}.npy\"\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def get_files_by_size(directory, file_size=None):\n",
    "    \"\"\"\n",
    "    Get a list of files in a directory that match the given size.\n",
    "\n",
    "    :param directory: The directory to search in.\n",
    "    :param file_size: The size to filter files by (in bytes).\n",
    "    :return: tuple of (list of all files, list of files that match the given size)\n",
    "    \"\"\"\n",
    "    # List to store the files with the given size\n",
    "    matching_files = []\n",
    "    all_files = []\n",
    "\n",
    "    # Iterate over the files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        # Check if it's a file and get its size\n",
    "        if os.path.isfile(filepath):\n",
    "            size = os.path.getsize(filepath)\n",
    "            all_files.append((filepath, size))\n",
    "            # If the file size matches, add it to the list\n",
    "            if file_size is not None and size == file_size:\n",
    "                matching_files.append(filepath)\n",
    "\n",
    "    return all_files, matching_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just a check of consistency with previous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping from one-hot encoded arrays to nucleotide symbols\n",
    "def decode_one_hot(one_hot_sequence, start_pos=None, window_size=None):\n",
    "    nucleotide_map = ['A', 'C', 'G', 'T']\n",
    "\n",
    "    if start_pos is not None and window_size is not None:\n",
    "        one_hot_sequence = one_hot_sequence[start_pos:start_pos + window_size]\n",
    "\n",
    "    decoded_sequence = ''.join(nucleotide_map[np.argmax(base)] for base in one_hot_sequence)\n",
    "    return decoded_sequence\n",
    "\n",
    "\n",
    "check_list = []\n",
    "for REGION_INDEX in range(100):\n",
    "\n",
    "    chrom  = query_regions[REGION_INDEX][0]\n",
    "    posmin = query_regions[REGION_INDEX][1] - 131071\n",
    "    posmax = query_regions[REGION_INDEX][2] + 131072\n",
    "    seq = np.load(f\"/grand/TFXcan/imlab/data/enformer_training_data/larger_window/human/chr{chrom}_{posmin}_{posmax}.npy\")\n",
    "    \n",
    "    short_seq = sequences[REGION_INDEX]\n",
    "    \n",
    "    check_bool = decode_one_hot(short_seq) == decode_one_hot(seq, start_pos=131072, window_size=131072)\n",
    "    check_list.append(check_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"test_human_393216.hdf5\"\n",
    "\"train_human_393216.hdf5\"\n",
    "\"test_mouse_393216.hdf5\"\n",
    "\"train_mouse_393216.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iterate over my seq files\n",
    "- Remove those with the wrong size\n",
    "- Link the sequence with the target, using the \"query_regions\"\n",
    "- Add them one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOUSE_DIR = \"/grand/TFXcan/imlab/data/enformer_training_data/larger_window/mouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_regions, sequences, targets = {}, {}, {}\n",
    "\n",
    "for dataset_tag in [\"train\", \"test\"]:\n",
    "    \n",
    "    file_path = f\"/grand/TFXcan/imlab/data/enformer_training_data/basenji_data_h5/no_groups_popseq_revised_order/{dataset_tag}_pop_seq.hdf5\"  # Replace with the actual file path\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as hdf:\n",
    "\n",
    "        query_regions[dataset_tag] = hdf['query_regions'][:]\n",
    "        sequences[dataset_tag]     = hdf['sequence']\n",
    "        targets[dataset_tag]       = hdf['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIGHT_FILESIZE = 12583040\n",
    "\n",
    "HUMAN_DIR = \"/grand/TFXcan/imlab/data/enformer_training_data/larger_window/human\"\n",
    "file_sizes, good_files = get_files_by_size(HUMAN_DIR, RIGHT_FILESIZE)\n",
    "good_files = set(good_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written sequence 0 to HDF5\n",
      "Written sequence 1000 to HDF5\n",
      "Written sequence 2000 to HDF5\n",
      "Written sequence 3000 to HDF5\n",
      "Written sequence 4000 to HDF5\n",
      "Written sequence 5000 to HDF5\n",
      "Written sequence 6000 to HDF5\n",
      "Written sequence 7000 to HDF5\n",
      "Written sequence 8000 to HDF5\n",
      "Written sequence 9000 to HDF5\n",
      "Written sequence 10000 to HDF5\n",
      "Written sequence 11000 to HDF5\n",
      "Written sequence 12000 to HDF5\n",
      "Written sequence 13000 to HDF5\n",
      "Written sequence 14000 to HDF5\n",
      "Written sequence 15000 to HDF5\n",
      "Written sequence 16000 to HDF5\n",
      "Written sequence 17000 to HDF5\n",
      "Written sequence 18000 to HDF5\n",
      "Written sequence 19000 to HDF5\n",
      "Written sequence 20000 to HDF5\n",
      "Written sequence 21000 to HDF5\n",
      "Written sequence 22000 to HDF5\n",
      "Written sequence 23000 to HDF5\n",
      "Written sequence 24000 to HDF5\n",
      "Written sequence 25000 to HDF5\n",
      "Written sequence 26000 to HDF5\n",
      "Written sequence 27000 to HDF5\n",
      "Written sequence 28000 to HDF5\n",
      "Written sequence 29000 to HDF5\n",
      "Written sequence 30000 to HDF5\n",
      "Written sequence 31000 to HDF5\n"
     ]
    }
   ],
   "source": [
    "for dataset_tag in ['train', 'test']:\n",
    "\n",
    "    # LENGTH = 1000\n",
    "\n",
    "    kk = { k:file for k, file in map_to_files(query_regions[dataset_tag]).items() if file in good_files }\n",
    "    hdf5_file_path = f\"/grand/TFXcan/imlab/data/enformer_training_data/larger_window/{dataset_tag}_human.hdf5\"\n",
    "    total_sequences = len(kk)\n",
    "    # total_sequences = LENGTH\n",
    "    sequence_length = 393216\n",
    "    nucleotides = 4\n",
    "    \n",
    "    with h5py.File(f\"/grand/TFXcan/imlab/data/enformer_training_data/basenji_data_h5/no_groups_popseq_revised_order/{dataset_tag}_pop_seq.hdf5\", \"r\") as old_hdf:\n",
    "        with h5py.File(hdf5_file_path, 'w') as new_hdf:\n",
    "\n",
    "            original_targets = old_hdf['target']\n",
    "\n",
    "            sequence_dataset = new_hdf.create_dataset(\n",
    "                'sequence', \n",
    "                shape=(total_sequences, sequence_length, nucleotides), \n",
    "                chunks=(1, sequence_length, nucleotides), \n",
    "                dtype='int8'\n",
    "            )\n",
    "    \n",
    "            target_dataset = new_hdf.create_dataset(\n",
    "                'target', \n",
    "                shape=(total_sequences, 896, 5313), \n",
    "                chunks=(1, 896, 5313), \n",
    "                dtype='float'\n",
    "            )\n",
    "        \n",
    "            for i, (k, v) in enumerate(kk.items()):\n",
    "\n",
    "                #if i == LENGTH:\n",
    "                 #   break\n",
    "    \n",
    "                one_slice_seq    = np.load(kk[k])\n",
    "                one_slice_target = original_targets[k]\n",
    "                    \n",
    "                # Write this slice into the dataset at position i\n",
    "                sequence_dataset[i, :, :] = one_slice_seq\n",
    "                target_dataset[i, :, :]   = one_slice_target\n",
    "                    \n",
    "                # Optionally print progress\n",
    "                if i % 1000 == 0:\n",
    "                    print(f\"Written sequence {i} to HDF5\")                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Create the dataset with chunks enabled (you can adjust the chunk size as needed)\n",
    "    sequence_dataset = hdf.create_dataset('sequence', \n",
    "                                          shape=(total_sequences, sequence_length, nucleotides), \n",
    "                                          chunks=(1, sequence_length, nucleotides), \n",
    "                                          dtype='int8')  # Assuming one-hot is encoded as int8\n",
    "    \n",
    "    # Now, let's assume you have slices of data from .npy files (or another source)\n",
    "    for i in range(total_sequences):\n",
    "        # Load one slice of sequence data at a time\n",
    "        # Here we simulate loading a slice; replace this with your actual loading mechanism\n",
    "        one_slice = np.random.randint(0, 2, size=(sequence_length, nucleotides))  # Random one-hot data\n",
    "        \n",
    "        # Write this slice into the dataset at position i\n",
    "        sequence_dataset[i, :, :] = one_slice\n",
    "        \n",
    "        # Optionally print progress\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Written sequence {i} to HDF5\")\n",
    "\n",
    "# After the loop, the full dataset will be saved in the HDF5 file\n",
    "print(\"All sequences written successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Enformer",
   "language": "python",
   "name": "enformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
